{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyQANdVB5HVp",
        "outputId": "2f1e42b4-8c7e-47a6-9ae0-22a9a1d91b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.7464788732394366\n",
            "KNN Accuracy: 0.7183098591549296\n",
            "NB Accuracy: 0.647887323943662\n",
            "DT Accuracy: 0.6619718309859155\n",
            "RF Accuracy: 0.704225352112676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Accuracy: 0.7605633802816901\n",
            "ADA Accuracy: 0.6619718309859155\n",
            "LR Accuracy: 0.7183098591549296\n",
            "Ensemble Accuracy: 0.7887323943661971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # For model persistence\n",
        "\n",
        "# Load data from CSV\n",
        "combined_data = pd.read_csv(\"combined_dataset.csv\")\n",
        "\n",
        "# Assuming 'Label' column contains the labels (1 for positive, 0 for negative)\n",
        "# If not, adjust the code accordingly\n",
        "X = combined_data.drop(['Name', 'Label'], axis=1)\n",
        "y = combined_data['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define multiple models\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'NB': GaussianNB(),\n",
        "    'DT': DecisionTreeClassifier(),\n",
        "    'RF': RandomForestClassifier(random_state=42),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'ADA': AdaBoostClassifier(random_state=42),\n",
        "    'LR': LogisticRegression(random_state=42)\n",
        "}\n",
        "\n",
        "# Train individual models and evaluate performance\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "# Ensemble by averaging predictions\n",
        "ensemble_preds = sum(model.predict(X_test) for model in models.values()) / len(models)\n",
        "ensemble_accuracy = accuracy_score(y_test, np.round(ensemble_preds))\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjvVJT3O8AXn",
        "outputId": "0d10604a-dacb-4648-ab74-6c42f7d25922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after oversampling: Counter({0: 173, 1: 173})\n",
            "SVM Accuracy: 0.7323943661971831\n",
            "KNN Accuracy: 0.704225352112676\n",
            "NB Accuracy: 0.676056338028169\n",
            "DT Accuracy: 0.647887323943662\n",
            "RF Accuracy: 0.7746478873239436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Accuracy: 0.7464788732394366\n",
            "ADA Accuracy: 0.676056338028169\n",
            "LR Accuracy: 0.7183098591549296\n",
            "Ensemble Accuracy: 0.7887323943661971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ensemble_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # For model persistence\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Load data from CSV\n",
        "combined_data = pd.read_csv(\"combined_dataset.csv\")\n",
        "\n",
        "# Assuming 'Label' column contains the labels (1 for positive, 0 for negative)\n",
        "# If not, adjust the code accordingly\n",
        "X = combined_data.drop(['Name', 'Label'], axis=1)\n",
        "y = combined_data['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Oversample the training data using SMOTE\n",
        "oversampler = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print the distribution of classes after oversampling\n",
        "print(\"Class distribution after oversampling:\", Counter(y_train_resampled))\n",
        "\n",
        "# Define multiple models\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'NB': GaussianNB(),\n",
        "    'DT': DecisionTreeClassifier(),\n",
        "    'RF': RandomForestClassifier(random_state=42),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'ADA': AdaBoostClassifier(random_state=42),\n",
        "    'LR': LogisticRegression(random_state=42)\n",
        "}\n",
        "\n",
        "# Train individual models and evaluate performance\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "\n",
        "    # Save individual models\n",
        "    joblib.dump(model, f'{model_name}_model.pkl')\n",
        "\n",
        "# Ensemble by averaging predictions\n",
        "ensemble_preds = sum(model.predict(X_test) for model in models.values()) / len(models)\n",
        "ensemble_accuracy = accuracy_score(y_test, np.round(ensemble_preds))\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
        "\n",
        "# Save ensemble predictions\n",
        "joblib.dump(ensemble_preds, 'ensemble_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsF7EMxA8XpX",
        "outputId": "eee3dde9-1d14-450d-cf6f-e6fe4a4502c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after oversampling: Counter({0: 173, 1: 173})\n",
            "New class distribution: Counter({0: 3000, 1: 3000})\n",
            "SVM Accuracy: 0.7605633802816901\n",
            "KNN Accuracy: 0.676056338028169\n",
            "NB Accuracy: 0.676056338028169\n",
            "DT Accuracy: 0.647887323943662\n",
            "RF Accuracy: 0.8309859154929577\n",
            "MLP Accuracy: 0.7887323943661971\n",
            "ADA Accuracy: 0.8028169014084507\n",
            "LR Accuracy: 0.704225352112676\n",
            "Ensemble Accuracy: 0.8169014084507042\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Load data from CSV\n",
        "combined_data = pd.read_csv(\"combined_dataset.csv\")\n",
        "\n",
        "# Assuming 'Label' column contains the labels (1 for positive, 0 for negative)\n",
        "# If not, adjust the code accordingly\n",
        "X = combined_data.drop(['Name', 'Label'], axis=1)\n",
        "y = combined_data['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Oversample the training data using SMOTE\n",
        "oversampler = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print the distribution of classes after oversampling\n",
        "print(\"Class distribution after oversampling:\", Counter(y_train_resampled))\n",
        "\n",
        "# Duplicate random rows to increase the dataset size to 3000 for both positive and negative data\n",
        "positive_indices = y_train_resampled[y_train_resampled == 1].index\n",
        "negative_indices = y_train_resampled[y_train_resampled == 0].index\n",
        "\n",
        "# Duplicate positive samples\n",
        "positive_duplicates = X_train_resampled.loc[positive_indices].sample(n=3000 - len(positive_indices), replace=True)\n",
        "X_train_resampled = pd.concat([X_train_resampled, positive_duplicates])\n",
        "y_train_resampled = pd.concat([y_train_resampled, pd.Series([1] * len(positive_duplicates))])\n",
        "\n",
        "# Duplicate negative samples\n",
        "negative_duplicates = X_train_resampled.loc[negative_indices].sample(n=3000 - len(negative_indices), replace=True)\n",
        "X_train_resampled = pd.concat([X_train_resampled, negative_duplicates])\n",
        "y_train_resampled = pd.concat([y_train_resampled, pd.Series([0] * len(negative_duplicates))])\n",
        "\n",
        "# Print the new class distribution\n",
        "print(\"New class distribution:\", Counter(y_train_resampled))\n",
        "\n",
        "# Define multiple models\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'NB': GaussianNB(),\n",
        "    'DT': DecisionTreeClassifier(),\n",
        "    'RF': RandomForestClassifier(random_state=42),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'ADA': AdaBoostClassifier(random_state=42),\n",
        "    'LR': LogisticRegression(random_state=42, max_iter=1000),\n",
        "}\n",
        "\n",
        "# Train individual models and evaluate performance\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "\n",
        "# Ensemble by averaging predictions\n",
        "ensemble_preds = sum(model.predict(X_test) for model in models.values()) / len(models)\n",
        "ensemble_accuracy = accuracy_score(y_test, np.round(ensemble_preds))\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-XaCaUlAo1C",
        "outputId": "fa94aac9-9011-49a0-aafe-d47ba08e1f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after oversampling: Counter({0: 173, 1: 173})\n",
            "New class distribution: Counter({0: 3000, 1: 3000})\n",
            "SVM Accuracy: 0.7464788732394366\n",
            "KNN Accuracy: 0.7183098591549296\n",
            "NB Accuracy: 0.7183098591549296\n",
            "DT Accuracy: 0.5774647887323944\n",
            "RF Accuracy: 0.7464788732394366\n",
            "MLP Accuracy: 0.7605633802816901\n",
            "ADA Accuracy: 0.676056338028169\n",
            "LR Accuracy: 0.7183098591549296\n",
            "Ensemble Accuracy: 0.7887323943661971\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Load data from CSV\n",
        "combined_data = pd.read_csv(\"combined_dataset.csv\")\n",
        "\n",
        "# Assuming 'Label' column contains the labels (1 for positive, 0 for negative)\n",
        "# If not, adjust the code accordingly\n",
        "X = combined_data.drop(['Name', 'Label'], axis=1)\n",
        "y = combined_data['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Oversample the training data using SMOTE\n",
        "oversampler = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Scale features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrames after scaling\n",
        "X_train_resampled_scaled_df = pd.DataFrame(X_train_resampled_scaled, columns=X_train.columns)\n",
        "\n",
        "# Print the distribution of classes after oversampling\n",
        "print(\"Class distribution after oversampling:\", Counter(y_train_resampled))\n",
        "\n",
        "# Duplicate random rows to increase the dataset size to 3000 for both positive and negative data\n",
        "positive_indices = y_train_resampled[y_train_resampled == 1].index\n",
        "negative_indices = y_train_resampled[y_train_resampled == 0].index\n",
        "\n",
        "# Duplicate positive samples\n",
        "positive_duplicates = X_train_resampled_scaled_df.loc[positive_indices].sample(n=3000 - len(positive_indices), replace=True)\n",
        "X_train_resampled_scaled_df = pd.concat([X_train_resampled_scaled_df, positive_duplicates])\n",
        "y_train_resampled = pd.concat([y_train_resampled, pd.Series([1] * len(positive_duplicates))])\n",
        "\n",
        "# Duplicate negative samples\n",
        "negative_duplicates = X_train_resampled_scaled_df.loc[negative_indices].sample(n=3000 - len(negative_indices), replace=True)\n",
        "X_train_resampled_scaled_df = pd.concat([X_train_resampled_scaled_df, negative_duplicates])\n",
        "y_train_resampled = pd.concat([y_train_resampled, pd.Series([0] * len(negative_duplicates))])\n",
        "\n",
        "# Print the new class distribution\n",
        "print(\"New class distribution:\", Counter(y_train_resampled))\n",
        "\n",
        "# Define multiple models\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'NB': GaussianNB(),\n",
        "    'DT': DecisionTreeClassifier(),\n",
        "    'RF': RandomForestClassifier(random_state=42),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'ADA': AdaBoostClassifier(random_state=42),\n",
        "    'LR': LogisticRegression(random_state=42, max_iter=1000),\n",
        "}\n",
        "\n",
        "# Train individual models and evaluate performance\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_resampled_scaled_df.values, y_train_resampled.values)\n",
        "    predictions = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "\n",
        "# Ensemble by averaging predictions\n",
        "ensemble_preds = sum(model.predict(X_test_scaled) for model in models.values()) / len(models)\n",
        "ensemble_accuracy = accuracy_score(y_test, np.round(ensemble_preds))\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y9IBGOGONAB",
        "outputId": "9f4fba45-6c63-4922-a81a-8a0ecc8dc591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after oversampling: Counter({1: 215, 0: 215})\n",
            "SVM Cross-Validation Accuracy: 0.7755000000000001\n",
            "SVM Test Set Accuracy: 0.8873239436619719\n",
            "KNN Cross-Validation Accuracy: 0.7454999999999999\n",
            "KNN Test Set Accuracy: 0.8591549295774648\n",
            "NB Cross-Validation Accuracy: 0.551\n",
            "NB Test Set Accuracy: 0.6619718309859155\n",
            "DT Cross-Validation Accuracy: 0.6725\n",
            "DT Test Set Accuracy: 1.0\n",
            "RF Cross-Validation Accuracy: 0.7985\n",
            "RF Test Set Accuracy: 1.0\n",
            "MLP Cross-Validation Accuracy: 0.782\n",
            "MLP Test Set Accuracy: 1.0\n",
            "ADA Cross-Validation Accuracy: 0.7440000000000001\n",
            "ADA Test Set Accuracy: 0.9014084507042254\n",
            "LR Cross-Validation Accuracy: 0.698\n",
            "LR Test Set Accuracy: 0.8169014084507042\n",
            "Ensemble Accuracy: 0.971830985915493\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "\n",
        "# Suppress ConvergenceWarnings\n",
        "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
        "\n",
        "# Load data from CSV\n",
        "combined_data = pd.read_csv(\"combined_dataset.csv\")\n",
        "\n",
        "# Assuming 'Label' column contains the labels (1 for positive, 0 for negative)\n",
        "# If not, adjust the code accordingly\n",
        "X = combined_data.drop(['Name', 'Label'], axis=1)\n",
        "y = combined_data['Label']\n",
        "\n",
        "# Oversample the data using SMOTE\n",
        "oversampler = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
        "\n",
        "# Scale features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "# Convert back to DataFrame after scaling\n",
        "X_resampled_scaled_df = pd.DataFrame(X_resampled_scaled, columns=X.columns)\n",
        "\n",
        "# Print the distribution of classes after oversampling\n",
        "print(\"Class distribution after oversampling:\", Counter(y_resampled))\n",
        "\n",
        "# Use StratifiedKFold for 100-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=100, shuffle=True, random_state=42)\n",
        "\n",
        "# Define multiple models\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'NB': GaussianNB(),\n",
        "    'DT': DecisionTreeClassifier(),\n",
        "    'RF': RandomForestClassifier(random_state=42),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'ADA': AdaBoostClassifier(random_state=42),\n",
        "    'LR': LogisticRegression(random_state=42, max_iter=1000),\n",
        "}\n",
        "\n",
        "# Train individual models and evaluate performance using cross-validation\n",
        "for model_name, model in models.items():\n",
        "    # Perform cross-validation\n",
        "    scores = cross_val_score(model, X_resampled_scaled_df, y_resampled, scoring='accuracy', cv=cv)\n",
        "    print(f\"{model_name} Cross-Validation Accuracy: {np.mean(scores)}\")\n",
        "\n",
        "    # Optionally, you can use the entire dataset for training\n",
        "    model.fit(X_resampled_scaled_df.values, y_resampled.values)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    predictions = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f\"{model_name} Test Set Accuracy: {accuracy}\")\n",
        "\n",
        "# Ensemble by averaging predictions\n",
        "ensemble_preds = sum(model.predict(X_test_scaled) for model in models.values()) / len(models)\n",
        "ensemble_accuracy = accuracy_score(y_test, np.round(ensemble_preds))\n",
        "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XR3d-tBgFxAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eaa9d80-d7b1-4bc4-803b-2b720516c6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after oversampling: Counter({0: 173, 1: 173})\n",
            "New class distribution: Counter({0: 3000, 1: 3000})\n",
            "Epoch 1/15\n",
            "150/150 [==============================] - 5s 19ms/step - loss: 0.7086 - accuracy: 0.5900 - val_loss: 0.7493 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/15\n",
            "150/150 [==============================] - 3s 18ms/step - loss: 0.6839 - accuracy: 0.6137 - val_loss: 0.7830 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/15\n",
            "150/150 [==============================] - 3s 20ms/step - loss: 0.6773 - accuracy: 0.6206 - val_loss: 0.8194 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/15\n",
            "150/150 [==============================] - 4s 25ms/step - loss: 0.6702 - accuracy: 0.6215 - val_loss: 0.8500 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/15\n",
            "150/150 [==============================] - 3s 19ms/step - loss: 0.6633 - accuracy: 0.6212 - val_loss: 0.8989 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/15\n",
            "150/150 [==============================] - 3s 20ms/step - loss: 0.6603 - accuracy: 0.6217 - val_loss: 0.8971 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/15\n",
            "150/150 [==============================] - 3s 17ms/step - loss: 0.6341 - accuracy: 0.6231 - val_loss: 0.8865 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/15\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 0.5824 - accuracy: 0.6198 - val_loss: 0.8398 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/15\n",
            "150/150 [==============================] - 4s 23ms/step - loss: 0.5306 - accuracy: 0.6227 - val_loss: 0.7815 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/15\n",
            "150/150 [==============================] - 3s 17ms/step - loss: 0.4922 - accuracy: 0.6231 - val_loss: 0.7103 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/15\n",
            "150/150 [==============================] - 2s 17ms/step - loss: 0.4739 - accuracy: 0.7098 - val_loss: 0.6654 - val_accuracy: 1.0000\n",
            "Epoch 12/15\n",
            "150/150 [==============================] - 3s 17ms/step - loss: 0.4341 - accuracy: 0.7827 - val_loss: 0.6223 - val_accuracy: 0.9925\n",
            "Epoch 13/15\n",
            "150/150 [==============================] - 4s 24ms/step - loss: 0.4054 - accuracy: 0.8142 - val_loss: 0.5793 - val_accuracy: 0.9917\n",
            "Epoch 14/15\n",
            "150/150 [==============================] - 3s 22ms/step - loss: 0.4341 - accuracy: 0.8025 - val_loss: 0.5417 - val_accuracy: 1.0000\n",
            "Epoch 15/15\n",
            "150/150 [==============================] - 3s 17ms/step - loss: 0.4218 - accuracy: 0.7940 - val_loss: 0.5190 - val_accuracy: 1.0000\n",
            "3/3 [==============================] - 0s 6ms/step\n",
            "Test Accuracy: 0.6619718309859155\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load data from CSV\n",
        "combined_data = pd.read_csv(\"combined_dataset.csv\")\n",
        "\n",
        "# Assuming 'Label' column contains the labels (1 for positive, 0 for negative)\n",
        "# If not, adjust the code accordingly\n",
        "X = combined_data.drop(['Name', 'Label'], axis=1)\n",
        "y = combined_data['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Oversample the training data using SMOTE\n",
        "oversampler = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Scale features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrames after scaling\n",
        "X_train_resampled_scaled_df = pd.DataFrame(X_train_resampled_scaled, columns=X_train.columns)\n",
        "\n",
        "# Print the distribution of classes after oversampling\n",
        "print(\"Class distribution after oversampling:\", Counter(y_train_resampled))\n",
        "\n",
        "# Duplicate random rows to increase the dataset size to 3000 for both positive and negative data\n",
        "positive_indices = y_train_resampled[y_train_resampled == 1].index\n",
        "negative_indices = y_train_resampled[y_train_resampled == 0].index\n",
        "\n",
        "# Duplicate positive samples\n",
        "positive_duplicates = X_train_resampled_scaled_df.loc[positive_indices].sample(n=3000 - len(positive_indices), replace=True)\n",
        "X_train_resampled_scaled_df = pd.concat([X_train_resampled_scaled_df, positive_duplicates])\n",
        "y_train_resampled = pd.concat([y_train_resampled, pd.Series([1] * len(positive_duplicates))])\n",
        "\n",
        "# Duplicate negative samples\n",
        "negative_duplicates = X_train_resampled_scaled_df.loc[negative_indices].sample(n=3000 - len(negative_indices), replace=True)\n",
        "X_train_resampled_scaled_df = pd.concat([X_train_resampled_scaled_df, negative_duplicates])\n",
        "y_train_resampled = pd.concat([y_train_resampled, pd.Series([0] * len(negative_duplicates))])\n",
        "\n",
        "# Print the new class distribution\n",
        "print(\"New class distribution:\", Counter(y_train_resampled))\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(1024, activation='relu', input_shape=(X_train_resampled_scaled_df.shape[1],)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with more epochs\n",
        "model.fit(X_train_resampled_scaled_df, y_train_resampled, epochs=15, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate on the test set\n",
        "predictions = model.predict(X_test_scaled)\n",
        "binary_predictions = np.round(predictions)\n",
        "accuracy = accuracy_score(y_test, binary_predictions)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XBBI4TgrFgxI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
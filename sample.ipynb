{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-20T17:45:09.130524700Z",
     "start_time": "2024-01-20T17:45:07.106697700Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'seq'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 57\u001B[0m\n\u001B[0;32m     54\u001B[0m all_sequences \u001B[38;5;241m=\u001B[39m moonlight_sequences \u001B[38;5;241m+\u001B[39m non_moonlight_sequences\n\u001B[0;32m     55\u001B[0m all_labels \u001B[38;5;241m=\u001B[39m moonlight_labels \u001B[38;5;241m+\u001B[39m non_moonlight_labels\n\u001B[1;32m---> 57\u001B[0m \u001B[38;5;28mprint\u001B[39m(all_sequences\u001B[38;5;241m.\u001B[39mseq)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'seq'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = str(self.sequences[idx].seq)\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        # Data Augmentation: Apply random mutations\n",
    "        sequence = self.apply_random_mutations(sequence)\n",
    "\n",
    "        encoding = self.tokenizer(sequence, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def apply_random_mutations(self, sequence):\n",
    "        # Implement your own data augmentation logic here\n",
    "        # For simplicity, you can randomly replace some amino acids with others\n",
    "        mutated_sequence = list(sequence)\n",
    "        for i in range(len(mutated_sequence)):\n",
    "            if random.random() < 0.1:  # Probability of mutation: 10%\n",
    "                mutated_sequence[i] = random.choice('ACDEFGHIKLMNPQRSTVWY')\n",
    "\n",
    "        return ''.join(mutated_sequence)\n",
    "\n",
    "# Load your datasets\n",
    "moonlight_sequences = list(SeqIO.parse(\"moonlight.fasta\", \"fasta\"))\n",
    "non_moonlight_sequences = list(SeqIO.parse(\"nonMP.fasta\", \"fasta\"))\n",
    "\n",
    "moonlight_labels = [1] * len(moonlight_sequences)\n",
    "non_moonlight_labels = [0] * len(non_moonlight_sequences)\n",
    "\n",
    "all_sequences = moonlight_sequences + non_moonlight_sequences\n",
    "all_labels = moonlight_labels + non_moonlight_labels\n",
    "\n",
    "print(all_sequences)\n",
    "\n",
    "# train_sequences, test_sequences, train_labels, test_labels = train_test_split(\n",
    "#     all_sequences, all_labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "# \n",
    "# train_sequences, val_sequences, train_labels, val_labels = train_test_split(\n",
    "#     train_sequences, train_labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "# \n",
    "# # Create datasets\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# \n",
    "# train_dataset = CustomDataset(train_sequences, train_labels, tokenizer, max_length=256)  # Increase max_length\n",
    "# val_dataset = CustomDataset(val_sequences, val_labels, tokenizer, max_length=256)  # Increase max_length\n",
    "# test_dataset = CustomDataset(test_sequences, test_labels, tokenizer, max_length=256)  # Increase max_length\n",
    "# \n",
    "# # Calculate class weights for data balancing\n",
    "# class_weights = torch.tensor([1.0 / sum(train_labels), 1.0 / (len(train_labels) - sum(train_labels))], dtype=torch.float32)\n",
    "# class_weights = class_weights.to(device)\n",
    "# \n",
    "# # Model\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "# model.to(device)\n",
    "# \n",
    "# # Optimizer and scheduler\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-6, weight_decay=1e-2)  # Experiment with a lower learning rate\n",
    "# total_steps = len(train_dataset) * 10  # Increase the number of epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "# \n",
    "# # Training loop\n",
    "# for epoch in range(10):  # Increase the number of epochs\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "# \n",
    "#     for batch in DataLoader(train_dataset, batch_size=8, shuffle=True):\n",
    "#         inputs = batch['input_ids'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "# \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "# \n",
    "#         # Apply class weights for data balancing\n",
    "#         loss = (loss * class_weights[labels]).mean()\n",
    "# \n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "# \n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "# \n",
    "#         total_loss += loss.item()\n",
    "# \n",
    "#     average_loss = total_loss / len(train_dataset)\n",
    "#     print(f'Epoch {epoch + 1}/10, Average Training Loss: {average_loss}')\n",
    "# \n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     val_predictions = []\n",
    "#     val_true_labels = []\n",
    "# \n",
    "#     for val_batch in DataLoader(val_dataset, batch_size=8, shuffle=False):\n",
    "#         val_inputs = val_batch['input_ids'].to(device)\n",
    "#         val_labels = val_batch['labels'].to(device)\n",
    "# \n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(val_inputs)\n",
    "# \n",
    "#         logits = val_outputs.logits\n",
    "#         predictions = torch.argmax(logits, dim=1)\n",
    "#         val_predictions.extend(predictions.cpu().numpy())\n",
    "#         val_true_labels.extend(val_labels.cpu().numpy())\n",
    "# \n",
    "#     val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "#     print(f'Epoch {epoch + 1}/10, Validation Accuracy: {val_accuracy}')\n",
    "# \n",
    "# # Testing\n",
    "# model.eval()\n",
    "# test_predictions = []\n",
    "# test_true_labels = []\n",
    "# \n",
    "# for test_batch in DataLoader(test_dataset, batch_size=8, shuffle=False):\n",
    "#     test_inputs = test_batch['input_ids'].to(device)\n",
    "#     test_labels = test_batch['labels'].to(device)\n",
    "# \n",
    "#     with torch.no_grad():\n",
    "#         test_outputs = model(test_inputs)\n",
    "# \n",
    "#     logits = test_outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=1)\n",
    "#     test_predictions.extend(predictions.cpu().numpy())\n",
    "#     test_true_labels.extend(test_labels.cpu().numpy())\n",
    "# \n",
    "# test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "# print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "41f72a1dcb3e824"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score, classification_report\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimblearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mover_sampling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RandomOverSampler\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mBio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SeqIO\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_and_preprocess_data\u001B[39m(file_path):\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\__init__.py:52\u001B[0m\n\u001B[0;32m     48\u001B[0m     sys\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPartial import of imblearn during the build process.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001B[39;00m\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;66;03m# process, as it may not be compiled yet\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     53\u001B[0m         combine,\n\u001B[0;32m     54\u001B[0m         ensemble,\n\u001B[0;32m     55\u001B[0m         exceptions,\n\u001B[0;32m     56\u001B[0m         metrics,\n\u001B[0;32m     57\u001B[0m         over_sampling,\n\u001B[0;32m     58\u001B[0m         pipeline,\n\u001B[0;32m     59\u001B[0m         tensorflow,\n\u001B[0;32m     60\u001B[0m         under_sampling,\n\u001B[0;32m     61\u001B[0m         utils,\n\u001B[0;32m     62\u001B[0m     )\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_version\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FunctionSampler\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mover-sampling and under-sampling.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_smote_enn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SMOTEENN\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_smote_tomek\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SMOTETomek\n\u001B[0;32m      8\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSMOTEENN\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSMOTETomek\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m clone\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_X_y\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseSampler\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mover_sampling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SMOTE\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mover_sampling\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseOverSampler\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:21\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmulticlass\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_classification_targets\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_sampling_strategy, check_target_type\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_param_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m validate_parameter_constraints\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ArraysTransformer\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mSamplerMixin\u001B[39;00m(BaseEstimator, metaclass\u001B[38;5;241m=\u001B[39mABCMeta):\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001B[0m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_param_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m generate_valid_param  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[0;32m    907\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_param_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m validate_parameter_constraints  \u001B[38;5;66;03m# noqa\u001B[39;00m\n\u001B[1;32m--> 908\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_param_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    909\u001B[0m     HasMethods,\n\u001B[0;32m    910\u001B[0m     Hidden,\n\u001B[0;32m    911\u001B[0m     Interval,\n\u001B[0;32m    912\u001B[0m     Options,\n\u001B[0;32m    913\u001B[0m     StrOptions,\n\u001B[0;32m    914\u001B[0m     _ArrayLikes,\n\u001B[0;32m    915\u001B[0m     _Booleans,\n\u001B[0;32m    916\u001B[0m     _Callables,\n\u001B[0;32m    917\u001B[0m     _CVObjects,\n\u001B[0;32m    918\u001B[0m     _InstancesOf,\n\u001B[0;32m    919\u001B[0m     _IterablesNotString,\n\u001B[0;32m    920\u001B[0m     _MissingValues,\n\u001B[0;32m    921\u001B[0m     _NoneConstraint,\n\u001B[0;32m    922\u001B[0m     _PandasNAConstraint,\n\u001B[0;32m    923\u001B[0m     _RandomStates,\n\u001B[0;32m    924\u001B[0m     _SparseMatrices,\n\u001B[0;32m    925\u001B[0m     _VerboseHelper,\n\u001B[0;32m    926\u001B[0m     make_constraint,\n\u001B[0;32m    927\u001B[0m     validate_params,\n\u001B[0;32m    928\u001B[0m )\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    sequences = []\n",
    "    with open(file_path, \"r\") as fasta_file:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            # Assuming the protein sequences are stored in the 'sequence' attribute\n",
    "            sequences.append(str(record.seq))\n",
    "    return sequences\n",
    "\n",
    "# Load and preprocess your data\n",
    "moonlight_data = load_and_preprocess_data(\"moonlight.fasta\")\n",
    "non_moonlight_data = load_and_preprocess_data(\"nonMP.fasta\")\n",
    "\n",
    "# Create labels for your data (1 for moonlight, 0 for non-moonlight)\n",
    "labels = [1] * len(moonlight_data) + [0] * len(non_moonlight_data)\n",
    "\n",
    "# Combine the data\n",
    "all_data = moonlight_data + non_moonlight_data\n",
    "\n",
    "# Oversample the minority class to match the majority class\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"minority\", random_state=42)\n",
    "\n",
    "X_data = np.array(all_data).reshape(-1, 1)\n",
    "y_data = np.array(labels)\n",
    "\n",
    "# Oversample the minority class to match the majority class\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_data, y_data)\n",
    "\n",
    "# Convert the reshaped data back to lists\n",
    "all_data_resampled = X_resampled.flatten().tolist()\n",
    "labels_resampled = y_resampled.tolist()\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_data_resampled, labels_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"Rostlab/prot_bert_bfd\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize and encode the protein sequences\n",
    "X_train_encoded = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "X_test_encoded = tokenizer(X_test, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "# Fine-tune the pre-trained model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train the model\n",
    "    outputs = model(**X_train_encoded, labels=y_train_tensor.unsqueeze(1))\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(**X_test_encoded)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Convert predictions to a list\n",
    "predictions = predictions.tolist()\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T08:40:23.852304800Z",
     "start_time": "2024-01-20T08:40:23.513708100Z"
    }
   },
   "id": "e17fffc1d721f05d",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() :\n",
    "    print(\"GPU\")\n",
    "else :\n",
    "    print(\"CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T06:54:32.538651400Z",
     "start_time": "2024-01-20T06:54:32.508196900Z"
    }
   },
   "id": "4cc88f55925163bd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the datasets\n",
    "moonlight_data = pd.read_csv('Moonlight.csv')\n",
    "nonmoonlight_data = pd.read_csv('NonMoonLight.csv')\n",
    "\n",
    "# Add a column indicating moonlight (1) or non-moonlight (0)\n",
    "moonlight_data['Label'] = 1\n",
    "nonmoonlight_data['Label'] = 0\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_data = pd.concat([moonlight_data, nonmoonlight_data])\n",
    "\n",
    "# Randomize the order of rows\n",
    "combined_data = combined_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save the combined and randomized dataset to a new CSV file\n",
    "combined_data.to_csv('combined_dataset.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T18:04:59.875760700Z",
     "start_time": "2024-01-20T18:04:58.568229200Z"
    }
   },
   "id": "d656d726e9687ed2",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.704225352112676\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the combined dataset\n",
    "dataset = pd.read_csv('combined_dataset.csv')\n",
    "\n",
    "# Drop the 'Name' column\n",
    "dataset = dataset.drop('Name', axis=1)\n",
    "\n",
    "# Define the features and labels\n",
    "X = dataset.drop('Label', axis=1)\n",
    "y = dataset['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T18:23:32.649510400Z",
     "start_time": "2024-01-20T18:23:32.577307500Z"
    }
   },
   "id": "7c3e9931ec5d9f8",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading vocab.txt:   0%|          | 0.00/81.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28c70d7e68e54306a8c8c797e66e0a31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff64aad1744e41798165823e002b6243"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1b02cc4ffcf428c85dece932cc7d8d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/361 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19b529d6d2044aebbc838263e37d028c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/1.68G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89decb1d1cd8480098343e3537fed6df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Load the combined dataset\n",
    "dataset = pd.read_csv('combined_dataset.csv')\n",
    "\n",
    "# Drop the 'Name' column\n",
    "dataset = dataset.drop('Name', axis=1)\n",
    "\n",
    "# Define the features and labels\n",
    "X = dataset.drop('Label', axis=1)\n",
    "y = dataset['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Concatenate amino acid sequences into a text sequence\n",
    "X_train_text = X_train.astype(str).apply(' '.join, axis=1).values\n",
    "X_test_text = X_test.astype(str).apply(' '.join, axis=1).values\n",
    "\n",
    "# Load pre-trained ProtBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd')\n",
    "model = BertForSequenceClassification.from_pretrained('Rostlab/prot_bert_bfd')\n",
    "\n",
    "# Tokenize and encode the training data\n",
    "X_train_tokens = tokenizer(list(X_train_text), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "y_train_tensor = torch.tensor(y_train.values)\n",
    "\n",
    "# Tokenize and encode the testing data\n",
    "X_test_tokens = tokenizer(list(X_test_text), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "y_test_tensor = torch.tensor(y_test.values)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(X_train_tokens['input_ids'], X_train_tokens['attention_mask'], y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tokens['input_ids'], X_test_tokens['attention_mask'], y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set up training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, all_preds)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-20T18:49:31.491510900Z"
    }
   },
   "id": "6f1e717c1f0885a8",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
